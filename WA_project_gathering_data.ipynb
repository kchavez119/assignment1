{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30dcf49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the tools that will be neccessary\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c4f8fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have all the links for reviews in a CSV lets retrive that\n",
    "Card_info = pd.read_csv(\"card_name_URL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "30c48fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets get those links from the dataframe into a list\n",
    "card_links = Card_info[\"Reviews URL\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bac44bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell python where chrome driver is wrt your current working directory\n",
    "path = \"chromedriver\"\n",
    "browser = webdriver.Chrome(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3bd4c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting some empty lists to store the data in the mean time\n",
    "data = []\n",
    "authors = []\n",
    "ratings = []\n",
    "dates = []\n",
    "review_texts = []\n",
    "review_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b48f2972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-147-e6dd51d294b0>:59: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  browser.find_element_by_xpath('//div[@data-testid=\"reviews__pagination-next-page\"]').click()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n",
      "page 1\n",
      "page 2\n",
      "page 3\n",
      "page 4\n",
      "page 5\n"
     ]
    }
   ],
   "source": [
    "#now lets gather all the data from each movie review link\n",
    "browser = webdriver.Chrome(path)\n",
    "for card in card_links:\n",
    "    pagelink = card\n",
    "    browser.get(pagelink)\n",
    "    time.sleep(2)\n",
    "    page_num = 5\n",
    "\n",
    "    for p in range(0, page_num):\n",
    "        print ('page',p+1)\n",
    "\n",
    "        # get all the review divs\n",
    "        page_source = browser.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "        reviews = soup.findAll('article', {'class':re.compile('pv3')})\n",
    "\n",
    "        # grab the information for each review\n",
    "        for review in reviews:\n",
    "\n",
    "            # initialize author, rating, date, review text, review title\n",
    "            author,rating,date,review_title,review_text ='NA','NA','NA','NA','NA'\n",
    "\n",
    "            #if there is critic name information, get it\n",
    "            author_ = review.find('div',{'class':re.compile('f5 lh-copy ck-black-60 pa0 mb3')})\n",
    "            if author_: \n",
    "                author = author_.text.strip()\n",
    "\n",
    "            # if there is rating information, get it\n",
    "            rating_ = review.findAll('svg',{'class':re.compile('fill-currentColor dib ck-primary-50')})\n",
    "            for i in range(0,len(rating_)):\n",
    "                rating = i + 1\n",
    "\n",
    "\n",
    "            # if there is date information, get it  \n",
    "            date_= review.find('span',{'class':re.compile(\"dib f5 lh-copy\")})\n",
    "            if date_: \n",
    "                date = date_.text.strip()\n",
    "\n",
    "            # if there is review title information, get it \n",
    "            reviewtitles = review.find('h5',{'class':re.compile('f4 lh-title ck-black-90 mb2 mt3')})\n",
    "            if reviewtitles: \n",
    "                review_title = reviewtitles.text.strip()\n",
    "    \n",
    "            # if there is review text information, get it \n",
    "            reviewtexts = review.find('p',{'class':re.compile('f4 lh-copy ma0')})\n",
    "            if reviewtexts: \n",
    "                review_text = reviewtexts.text\n",
    "            \n",
    "            #lets add the data to a list\n",
    "            authors.append(author)\n",
    "            ratings.append(rating)\n",
    "            dates.append(date)\n",
    "            review_titles.append(review_title)\n",
    "            review_texts.append(review_text)\n",
    "        \n",
    "        #moving to next page and setting a parameter incase there is no next page\n",
    "        try:\n",
    "            if p < page_num:\n",
    "                browser.find_element_by_xpath('//div[@data-testid=\"reviews__pagination-next-page\"]').click()\n",
    "                time.sleep(2)\n",
    "        except:\n",
    "            continue\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7c76bc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Authors</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Date</th>\n",
       "      <th>review_titles</th>\n",
       "      <th>review_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>Credit Karma Member</td>\n",
       "      <td>5</td>\n",
       "      <td>Apr 16, 2017</td>\n",
       "      <td>Great Card, fits my needs</td>\n",
       "      <td>Being 4yrs post bankruptcy, I was surprised to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3620</th>\n",
       "      <td>Credit Karma Member</td>\n",
       "      <td>1</td>\n",
       "      <td>Jan 27, 2017</td>\n",
       "      <td>Not worth it !!</td>\n",
       "      <td>I apply for this credit card since my credit s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>Credit Karma Member</td>\n",
       "      <td>5</td>\n",
       "      <td>Jan 07, 2017</td>\n",
       "      <td>Best Cash Back in Every Day Spending</td>\n",
       "      <td>This card loses a lot of people's interest bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>Credit Karma Member</td>\n",
       "      <td>2</td>\n",
       "      <td>Dec 19, 2016</td>\n",
       "      <td>Barclays is cheap on credit lines</td>\n",
       "      <td>Every time Barclays gets a credit line request...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>Credit Karma Member</td>\n",
       "      <td>5</td>\n",
       "      <td>Nov 22, 2016</td>\n",
       "      <td>Great Card</td>\n",
       "      <td>While I understand the comments others say abo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Authors  Ratings          Date  \\\n",
       "3619  Credit Karma Member        5  Apr 16, 2017   \n",
       "3620  Credit Karma Member        1  Jan 27, 2017   \n",
       "3621  Credit Karma Member        5  Jan 07, 2017   \n",
       "3622  Credit Karma Member        2  Dec 19, 2016   \n",
       "3623  Credit Karma Member        5  Nov 22, 2016   \n",
       "\n",
       "                             review_titles  \\\n",
       "3619             Great Card, fits my needs   \n",
       "3620                       Not worth it !!   \n",
       "3621  Best Cash Back in Every Day Spending   \n",
       "3622     Barclays is cheap on credit lines   \n",
       "3623                            Great Card   \n",
       "\n",
       "                                           review_texts  \n",
       "3619  Being 4yrs post bankruptcy, I was surprised to...  \n",
       "3620  I apply for this credit card since my credit s...  \n",
       "3621  This card loses a lot of people's interest bec...  \n",
       "3622  Every time Barclays gets a credit line request...  \n",
       "3623  While I understand the comments others say abo...  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding the data to a data frame\n",
    "data = {\n",
    "    'Authors': authors,\n",
    "    'Ratings': ratings,\n",
    "    'Date' : dates,\n",
    "    'review_titles': review_titles,\n",
    "    'review_texts': review_texts\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# check the shape of our dataframe\n",
    "df.shape\n",
    "# take a glimpse \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "aac22690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets delete any duplicate rows that may have been saved and reset the index\n",
    "card_review_data = df.drop_duplicates()\n",
    "card_review_data = card_review_data.reset_index()\n",
    "card_review_data = card_review_data.drop(['index'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "07178fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets save that data we just gathered to a csv file\n",
    "pd.DataFrame(card_review_data).to_csv(\"creditcardreviews.csv\", encoding='utf-8',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
